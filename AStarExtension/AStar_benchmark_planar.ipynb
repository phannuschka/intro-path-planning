{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f74e2ad3",
   "metadata": {},
   "source": [
    "# Evaluation Planar Robot\n",
    "\n",
    "In this notebook we evaluate the extended A* using a planar robot with 3, 6 and 9 DoF. First, let us take a look at the environments used for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d82256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.IPBenchmark import Benchmark \n",
    "import evaluation.robotic_arm.IPTestSuite_robotic_arm_3_DoF as ts\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from evaluation.robotic_arm.PlotEnvironments import visualizeBenchmark\n",
    "\n",
    "for benchmark in ts.benchList:\n",
    "        benchmark: Benchmark\n",
    "        print(f\"----- benchmark: {benchmark.name} -----\")\n",
    "        fig, ax = visualizeBenchmark(benchmark)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17496612",
   "metadata": {},
   "source": [
    "## Evaluation Code\n",
    "\n",
    "In this section you can find some code for evaluating A*, loading existing results and plotting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c7542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import networkx as nx\n",
    "from typing import List, Dict\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), 'core'))\n",
    "sys.path.append(os.path.join(os.getcwd(), 'evaluation/robotic_arm'))\n",
    "\n",
    "from core.IPAStarExtended import AStar\n",
    "from evaluation.robotic_arm.Benchmarking import get_config_dir_name, evaluate\n",
    "import evaluation.robotic_arm.IPTestSuite_robotic_arm_3_DoF as ts3\n",
    "import evaluation.robotic_arm.IPTestSuite_robotic_arm_6_DoF as ts6\n",
    "import evaluation.robotic_arm.IPTestSuite_robotic_arm_9_DoF as ts9\n",
    "from core.IPLazyPRM import LazyPRM\n",
    "\n",
    "testSuits = {3: ts3, 6: ts6, 9: ts9}\n",
    "\n",
    "def get_evaluation_results(configs: List[Dict], algorithm: str = \"astar\", dump: bool = True):\n",
    "    results = []\n",
    "\n",
    "    for i, config in enumerate(configs):\n",
    "        print(f\"----- config {i + 1} of {len(configs)} -----\")\n",
    "\n",
    "        config_results = {}\n",
    "\n",
    "        ts = testSuits[config[\"dof\"]]\n",
    "        benchmarks = [ts.benchList[i] for i in config[\"benchmarks\"]]\n",
    "        for benchmark in benchmarks:\n",
    "            print(f\"----- benchmark: {benchmark.name} -----\")\n",
    "            benchmark_results = {}\n",
    "\n",
    "            dir_name = get_config_dir_name(config=config, benchmark_name=benchmark.name, algorithm=algorithm)\n",
    "\n",
    "            if os.path.exists(dir_name):\n",
    "                # load evaluation results if they exist.\n",
    "\n",
    "                match algorithm:\n",
    "                    case \"astar\":\n",
    "                        solver = AStar(benchmark.collisionChecker)\n",
    "\n",
    "                    case \"prm\":\n",
    "                        solver = LazyPRM(benchmark.collisionChecker)\n",
    "                \n",
    "                solver.start = benchmark.startList[0]\n",
    "                solver.goal = benchmark.goalList[0]\n",
    "                with open(f'{dir_name}/graph.json') as f:\n",
    "                    graph = nx.node_link_graph(json.load(f), edges=\"links\")\n",
    "                    solver.graph = graph\n",
    "\n",
    "                with open(f'{dir_name}/stats.json') as f:\n",
    "                    stats = json.load(f)\n",
    "\n",
    "                with open(f'{dir_name}/solution.json') as f:\n",
    "                    solution = json.load(f)\n",
    "\n",
    "            else:\n",
    "                # do evaluation\n",
    "\n",
    "                stats, solution, solver = evaluate(config=config, benchmark=benchmark, algorithm=algorithm, dump=dump)\n",
    "\n",
    "            \n",
    "            benchmark_results[\"stats\"] = stats\n",
    "            benchmark_results[\"solution\"] = solution\n",
    "            benchmark_results[\"solver\"] = solver\n",
    "\n",
    "            config_results[benchmark.name] = benchmark_results\n",
    "        results.append((config, config_results))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99296a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_line(results: List, x_axis_values: List, x_axis_title: str):\n",
    "    # plot execution time\n",
    "    plot_lines = {}\n",
    "    for bench_name in results[0][1].keys():\n",
    "        plot_lines[bench_name] = []\n",
    "\n",
    "    for _, config_results in results:\n",
    "        for bench_name, benchmark_results in config_results.items():\n",
    "            plot_lines[bench_name].append(benchmark_results[\"stats\"][\"execution_time\"])\n",
    "\n",
    "    for bench_name in results[0][1].keys():\n",
    "        plt.plot(x_axis_values, plot_lines[bench_name], \"o-\", label = bench_name)\n",
    "    plt.legend()\n",
    "    plt.title(\"Execution Time\")\n",
    "    plt.xlabel(x_axis_title)\n",
    "    plt.ylabel(\"Execution Time [s]\")\n",
    "    plt.show()\n",
    "\n",
    "    # plot roadmap size\n",
    "    plot_lines = {}\n",
    "    for bench_name in results[0][1].keys():\n",
    "        plot_lines[bench_name] = []\n",
    "\n",
    "    for _, config_results in results:\n",
    "        for bench_name, benchmark_results in config_results.items():\n",
    "            plot_lines[bench_name].append(benchmark_results[\"stats\"][\"road_map_size\"])\n",
    "\n",
    "    for bench_name in results[0][1].keys():\n",
    "        plt.plot(x_axis_values, plot_lines[bench_name], \"o-\", label = bench_name)\n",
    "    plt.legend()\n",
    "    plt.title(\"Roadmap Size\")\n",
    "    plt.xlabel(x_axis_title)\n",
    "    plt.ylabel(\"Roadmap Size\")\n",
    "    plt.show()\n",
    "\n",
    "    # plot nume nodes in solution path\n",
    "    plot_lines = {}\n",
    "    for bench_name in results[0][1].keys():\n",
    "        plot_lines[bench_name] = []\n",
    "\n",
    "    for _, config_results in results:\n",
    "        for bench_name, benchmark_results in config_results.items():\n",
    "            if (benchmark_results[\"stats\"][\"num_nodes_solution_path\"] >= 0):\n",
    "                plot_lines[bench_name].append(benchmark_results[\"stats\"][\"num_nodes_solution_path\"])\n",
    "            else:\n",
    "                plot_lines[bench_name].append(np.nan)\n",
    "\n",
    "    for bench_name in results[0][1].keys():\n",
    "        plt.plot(x_axis_values, plot_lines[bench_name], \"o-\", label = bench_name)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title(\"Number of Nodes in Solution Path\")\n",
    "    plt.xlabel(x_axis_title)\n",
    "    plt.ylabel(\"Number of Nodes in Solution Path\")\n",
    "    plt.show()\n",
    "\n",
    "    # plot solution path length\n",
    "    plot_lines = {}\n",
    "    for bench_name in results[0][1].keys():\n",
    "        plot_lines[bench_name] = []\n",
    "\n",
    "    for _, config_results in results:\n",
    "        for bench_name, benchmark_results in config_results.items():\n",
    "            if (benchmark_results[\"stats\"][\"solution_path_length\"] >= 0):\n",
    "                plot_lines[bench_name].append(benchmark_results[\"stats\"][\"solution_path_length\"])\n",
    "            else:\n",
    "                plot_lines[bench_name].append(np.nan)\n",
    "\n",
    "    for bench_name in results[0][1].keys():\n",
    "        plt.plot(x_axis_values, plot_lines[bench_name], \"o-\", label = bench_name)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.title(\"Solution Path Length\")\n",
    "    plt.xlabel(x_axis_title)\n",
    "    plt.ylabel(\"Solution Path Length\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d953b9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_bar(results: List, x_axis_values: List, x_axis_title: str):\n",
    "    # Get benchmark names\n",
    "    bench_names = list(results[0][1].keys())\n",
    "    n_benchmarks = len(bench_names)\n",
    "    n_configs = len(x_axis_values)\n",
    "    \n",
    "    # Set up bar positioning\n",
    "    bar_width = 0.8 / n_configs  # Total width of 0.8, divided by number of configs\n",
    "    x_positions = np.arange(n_benchmarks)\n",
    "    \n",
    "    # Plot execution time\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, x_val in enumerate(x_axis_values):\n",
    "        execution_times = []\n",
    "        for bench_name in bench_names:\n",
    "            _, config_results = results[i]\n",
    "            execution_times.append(config_results[bench_name][\"stats\"][\"execution_time\"])\n",
    "        \n",
    "        offset = (i - (n_configs - 1) / 2) * bar_width\n",
    "        plt.bar(x_positions + offset, execution_times, bar_width, \n",
    "                label=f'{x_axis_title}: {x_val}', alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Benchmarks')\n",
    "    plt.ylabel('Execution Time [s]')\n",
    "    plt.title('Execution Time by Benchmark')\n",
    "    plt.xticks(x_positions, bench_names, rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot roadmap size\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, x_val in enumerate(x_axis_values):\n",
    "        roadmap_sizes = []\n",
    "        for bench_name in bench_names:\n",
    "            _, config_results = results[i]\n",
    "            roadmap_sizes.append(config_results[bench_name][\"stats\"][\"road_map_size\"])\n",
    "        \n",
    "        offset = (i - (n_configs - 1) / 2) * bar_width\n",
    "        plt.bar(x_positions + offset, roadmap_sizes, bar_width, \n",
    "                label=f'{x_axis_title}: {x_val}', alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Benchmarks')\n",
    "    plt.ylabel('Roadmap Size')\n",
    "    plt.title('Roadmap Size by Benchmark')\n",
    "    plt.xticks(x_positions, bench_names, rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot number of nodes in solution path\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, x_val in enumerate(x_axis_values):\n",
    "        num_nodes = []\n",
    "        for bench_name in bench_names:\n",
    "            _, config_results = results[i]\n",
    "            val = config_results[bench_name][\"stats\"][\"num_nodes_solution_path\"]\n",
    "            num_nodes.append(val if val >= 0 else np.nan)\n",
    "        \n",
    "        offset = (i - (n_configs - 1) / 2) * bar_width\n",
    "        plt.bar(x_positions + offset, num_nodes, bar_width, \n",
    "                label=f'{x_axis_title}: {x_val}', alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Benchmarks')\n",
    "    plt.ylabel('Number of Nodes in Solution Path')\n",
    "    plt.title('Number of Nodes in Solution Path by Benchmark')\n",
    "    plt.xticks(x_positions, bench_names, rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot solution path length\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, x_val in enumerate(x_axis_values):\n",
    "        path_lengths = []\n",
    "        for bench_name in bench_names:\n",
    "            _, config_results = results[i]\n",
    "            val = config_results[bench_name][\"stats\"][\"solution_path_length\"]\n",
    "            path_lengths.append(val if val >= 0 else np.nan)\n",
    "        \n",
    "        offset = (i - (n_configs - 1) / 2) * bar_width\n",
    "        plt.bar(x_positions + offset, path_lengths, bar_width, \n",
    "                label=f'{x_axis_title}: {x_val}', alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Benchmarks')\n",
    "    plt.ylabel('Solution Path Length')\n",
    "    plt.title('Solution Path Length by Benchmark')\n",
    "    plt.xticks(x_positions, bench_names, rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a444b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "from IPPlanarManipulator import PlanarRobot\n",
    "\n",
    "#%matplotlib widget\n",
    "\n",
    "def interpolate_line(startPos, endPos, step_l):\n",
    "    steps = []\n",
    "    line = np.array(endPos) - np.array(startPos)\n",
    "    line_l = np.linalg.norm(line)\n",
    "    step = line / line_l * step_l\n",
    "    n_steps = np.floor(line_l / step_l).astype(np.int32)\n",
    "    c_step = np.array(startPos)\n",
    "    for i in range(n_steps):\n",
    "        steps.append(copy.deepcopy(c_step))\n",
    "        c_step += step\n",
    "    if not (c_step == np.array(endPos)).all():\n",
    "        steps.append(np.array(endPos))\n",
    "    return steps\n",
    "\n",
    "matplotlib.rcParams['animation.embed_limit'] = 64\n",
    "def animateSolution(planner, environment, solution, dof):\n",
    "    _planner = planner\n",
    "    _environment = environment\n",
    "    _solution = solution\n",
    "    \n",
    "    fig_local = plt.figure(figsize=(7, 7))\n",
    "    ax = fig_local.add_subplot(1, 1, 1)\n",
    "    ## get positions for solution\n",
    "    solution_pos = [_planner.graph.nodes[node]['pos'] for node in _solution]\n",
    "    ## interpolate to obtain a smoother movement\n",
    "    i_solution_pos = [solution_pos[0]]\n",
    "    for i in range(1, len(solution_pos)):\n",
    "        segment_s = solution_pos[i-1]\n",
    "        segment_e = solution_pos[i]\n",
    "        i_solution_pos = i_solution_pos + interpolate_line(segment_s, segment_e, 0.1)[1:]\n",
    "\n",
    "    ## animate\n",
    "    frames = len(i_solution_pos)\n",
    "    \n",
    "    def animate(t):\n",
    "        ## clear taks space figure\n",
    "        ax.cla()\n",
    "        ## fix figure size\n",
    "        ax.set_xlim([-3,3])\n",
    "        ax.set_ylim([-3,3])\n",
    "        ## draw obstacles\n",
    "        _environment.drawObstacles(ax, inWorkspace = True)\n",
    "        ## update robot position\n",
    "        pos = i_solution_pos[t]\n",
    "        _environment.kin_chain.move(pos)\n",
    "        planarRobotVisualize(_environment.kin_chain, ax)\n",
    "\n",
    "    ani = matplotlib.animation.FuncAnimation(fig_local, animate, frames=frames)\n",
    "    html = HTML(ani.to_jshtml())\n",
    "    display(html)\n",
    "    plt.close()\n",
    "\n",
    "def planarRobotVisualize(kin_chain, ax):\n",
    "    joint_positions = kin_chain.get_transforms()\n",
    "    for i in range(1, len(joint_positions)):\n",
    "        xs = [joint_positions[i-1][0], joint_positions[i][0]]\n",
    "        ys = [joint_positions[i-1][1], joint_positions[i][1]]\n",
    "        ax.plot(xs, ys, color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8239eb",
   "metadata": {},
   "source": [
    "## Evaluation: Discretization\n",
    "\n",
    "In this section we will evaluate the influence of the discretization on A*. \n",
    "For computational cost reasons, we evaluate the higher DoF only with lower numbers of discretization steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955247e6",
   "metadata": {},
   "source": [
    "### 3DoF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5eca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "disc_values = [4, 6, 10, 20, 50]\n",
    "\n",
    "configs = []\n",
    "for disc in disc_values:\n",
    "    disc_config = dict()\n",
    "    disc_config[\"dof\"] = 3\n",
    "    disc_config[\"lowLimits\"] = [-2 *np.pi for _ in range(disc_config[\"dof\"])]\n",
    "    disc_config[\"highLimits\"] = [2 *np.pi for _ in range(disc_config[\"dof\"])]\n",
    "    disc_config[\"discretization\"] = [disc for _ in range(disc_config[\"dof\"])]\n",
    "    disc_config[\"w\"] = .5\n",
    "    disc_config[\"heuristic\"]  = \"euclidean\"\n",
    "    disc_config[\"reopen\"] = True\n",
    "    disc_config[\"check_connection\"] = True\n",
    "    disc_config[\"lazy_check_connection\"] = True\n",
    "    disc_config[\"benchmarks\"] = [0, 1, 2]\n",
    "\n",
    "    configs.append(disc_config)\n",
    "\n",
    "results = get_evaluation_results(configs=configs)\n",
    "\n",
    "plot_results_line(results=results, x_axis_values=disc_values, x_axis_title=\"Discretization Steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298a6a7b",
   "metadata": {},
   "source": [
    "### 6DoF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debfa00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "disc_values = [4, 6, 10, 20]\n",
    "\n",
    "configs = []\n",
    "for disc in disc_values:\n",
    "    disc_config = dict()\n",
    "    disc_config[\"dof\"] = 6\n",
    "    disc_config[\"lowLimits\"] = [-2 *np.pi for _ in range(disc_config[\"dof\"])]\n",
    "    disc_config[\"highLimits\"] = [2 *np.pi for _ in range(disc_config[\"dof\"])]\n",
    "    disc_config[\"discretization\"] = [disc for _ in range(disc_config[\"dof\"])]\n",
    "    disc_config[\"w\"] = .5\n",
    "    disc_config[\"heuristic\"]  = \"euclidean\"\n",
    "    disc_config[\"reopen\"] = True\n",
    "    disc_config[\"check_connection\"] = True\n",
    "    disc_config[\"lazy_check_connection\"] = True\n",
    "    disc_config[\"benchmarks\"] = [0, 1, 2]\n",
    "\n",
    "    configs.append(disc_config)\n",
    "\n",
    "results = get_evaluation_results(configs=configs)\n",
    "\n",
    "plot_results_line(results=results, x_axis_values=disc_values, x_axis_title=\"Discretization Steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32433cbb",
   "metadata": {},
   "source": [
    "### 9DoF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b9062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "disc_values = [4, 6]\n",
    "\n",
    "configs = []\n",
    "for disc in disc_values:\n",
    "    disc_config = dict()\n",
    "    disc_config[\"dof\"] = 9\n",
    "    disc_config[\"lowLimits\"] = [-2 *np.pi for _ in range(disc_config[\"dof\"])]\n",
    "    disc_config[\"highLimits\"] = [2 *np.pi for _ in range(disc_config[\"dof\"])]\n",
    "    disc_config[\"discretization\"] = [disc for _ in range(disc_config[\"dof\"])]\n",
    "    disc_config[\"w\"] = .5\n",
    "    disc_config[\"heuristic\"]  = \"euclidean\"\n",
    "    disc_config[\"reopen\"] = True\n",
    "    disc_config[\"check_connection\"] = True\n",
    "    disc_config[\"lazy_check_connection\"] = True\n",
    "    disc_config[\"benchmarks\"] = [0, 1, 2]\n",
    "\n",
    "    configs.append(disc_config)\n",
    "\n",
    "results = get_evaluation_results(configs=configs)\n",
    "\n",
    "plot_results_line(results=results, x_axis_values=disc_values, x_axis_title=\"Discretization Steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17151ba6",
   "metadata": {},
   "source": [
    "The execution time generally increases with the number of discretization steps. For higher numbers of DoF, this increase is so rapid, that we had to choose different maximum numbers of discretization steps for each robot arm. This was expected, because the upper bound for the roadmap size grows exponentially with the DoF.\n",
    "\n",
    "There is no clear trend we can see in the path length. But we would expect that for higher numbers of discretization, the path length would converge to one minimal path length for each combination of DoF and benchmark environment. However, our computational resources do not allow us to show this experimentally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd90fbc7",
   "metadata": {},
   "source": [
    "## Evaluation: Weight w\n",
    "\n",
    "In this section we will evaluate the influence of the weight w. For this experiment we use 3 DoF and 50 discretization steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068a4367",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w_values = [0.5, 0.75, 1]\n",
    "\n",
    "configs = []\n",
    "for w in w_values:\n",
    "        w_config  = dict()\n",
    "        w_config[\"dof\"] = 3\n",
    "        w_config[\"lowLimits\"] = [-2 *np.pi for _ in range(w_config[\"dof\"])]\n",
    "        w_config[\"highLimits\"] = [2 *np.pi for _ in range(w_config[\"dof\"])]\n",
    "        w_config[\"discretization\"] = [50 for _ in range(w_config[\"dof\"])]\n",
    "        w_config[\"w\"] = w\n",
    "        w_config[\"heuristic\"]  = \"euclidean\"\n",
    "        w_config[\"reopen\"] = True\n",
    "        w_config[\"check_connection\"] = True\n",
    "        w_config[\"lazy_check_connection\"] = True\n",
    "        w_config[\"benchmarks\"] = [0, 1, 2]\n",
    "\n",
    "        configs.append(w_config)\n",
    "\n",
    "results = get_evaluation_results(configs=configs)\n",
    "\n",
    "plot_results_line(results=results, x_axis_values=w_values, x_axis_title=\"Weight w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fef357",
   "metadata": {},
   "source": [
    "Increasing the w value leads to significant improvements in the execution time in this experiment. Of course, a w value that is higher than 0.5 also means that the solution is not optimal anymore, which manifests in an increase of the path length. How much the path length changes seems to be highly dependent on the environment. As a general trend, the path length of more complex scenes seems to suffer more severely when w is increased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268190b5",
   "metadata": {},
   "source": [
    "## Evaluation: Comparison to Lazy PRM\n",
    "\n",
    "In this section we compare A* to Lazy PRM.\n",
    "\n",
    "For Lazy PRM we use the parameter values:     \n",
    "initialRoadmapSize = 500     \n",
    "updateRoadmapSize  = 50    \n",
    "kNearest = 20       \n",
    "maxIterations = 100     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a32a357",
   "metadata": {},
   "source": [
    "### 3DoF\n",
    "\n",
    "In this experiment we use 50 discretization steps for A*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5818ef8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = [\"A*\", \"Lazy PRM\"]\n",
    "\n",
    "configs = []\n",
    "\n",
    "astar_config  = dict()\n",
    "astar_config[\"dof\"] = 3\n",
    "astar_config[\"lowLimits\"] = [-2 *np.pi for _ in range(astar_config[\"dof\"])]\n",
    "astar_config[\"highLimits\"] = [2 *np.pi for _ in range(astar_config[\"dof\"])]\n",
    "astar_config[\"discretization\"] = [50 for _ in range(astar_config[\"dof\"])]\n",
    "astar_config[\"w\"] = 0.5\n",
    "astar_config[\"heuristic\"]  = \"euclidean\"\n",
    "astar_config[\"reopen\"] = True\n",
    "astar_config[\"check_connection\"] = True\n",
    "astar_config[\"lazy_check_connection\"] = True\n",
    "astar_config[\"benchmarks\"] = [0, 1, 2]\n",
    "\n",
    "configs.append(astar_config)\n",
    "\n",
    "\n",
    "results_3DoF = get_evaluation_results(configs=configs, algorithm=\"astar\")\n",
    "results_3DoF.extend(get_evaluation_results(configs=configs, algorithm=\"prm\"))\n",
    "\n",
    "plot_results_bar(results=results_3DoF, x_axis_values=algorithms, x_axis_title=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de167abe",
   "metadata": {},
   "source": [
    "### 6DoF\n",
    "\n",
    "In this experiment we use 20 discretization steps for A*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bef66b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = [\"A*\", \"Lazy PRM\"]\n",
    "\n",
    "configs = []\n",
    "\n",
    "astar_config  = dict()\n",
    "astar_config[\"dof\"] = 6\n",
    "astar_config[\"lowLimits\"] = [-2 *np.pi for _ in range(astar_config[\"dof\"])]\n",
    "astar_config[\"highLimits\"] = [2 *np.pi for _ in range(astar_config[\"dof\"])]\n",
    "astar_config[\"discretization\"] = [20 for _ in range(astar_config[\"dof\"])]\n",
    "astar_config[\"w\"] = 0.5\n",
    "astar_config[\"heuristic\"]  = \"euclidean\"\n",
    "astar_config[\"reopen\"] = True\n",
    "astar_config[\"check_connection\"] = True\n",
    "astar_config[\"lazy_check_connection\"] = True\n",
    "astar_config[\"benchmarks\"] = [0, 1, 2]\n",
    "\n",
    "configs.append(astar_config)\n",
    "\n",
    "\n",
    "results_6DoF = get_evaluation_results(configs=configs, algorithm=\"astar\")\n",
    "results_6DoF.extend(get_evaluation_results(configs=configs, algorithm=\"prm\"))\n",
    "\n",
    "plot_results_bar(results=results_6DoF, x_axis_values=algorithms, x_axis_title=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def1ae0e",
   "metadata": {},
   "source": [
    "### 9DoF\n",
    "\n",
    "In this experiment we use 6 discretization steps for A*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f062fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = [\"A*\", \"Lazy PRM\"]\n",
    "\n",
    "configs = []\n",
    "\n",
    "astar_config  = dict()\n",
    "astar_config[\"dof\"] = 9\n",
    "astar_config[\"lowLimits\"] = [-2 *np.pi for _ in range(astar_config[\"dof\"])]\n",
    "astar_config[\"highLimits\"] = [2 *np.pi for _ in range(astar_config[\"dof\"])]\n",
    "astar_config[\"discretization\"] = [6 for _ in range(astar_config[\"dof\"])]\n",
    "astar_config[\"w\"] = 0.5\n",
    "astar_config[\"heuristic\"]  = \"euclidean\"\n",
    "astar_config[\"reopen\"] = True\n",
    "astar_config[\"check_connection\"] = True\n",
    "astar_config[\"lazy_check_connection\"] = True\n",
    "astar_config[\"benchmarks\"] = [0, 1, 2]\n",
    "\n",
    "configs.append(astar_config)\n",
    "\n",
    "\n",
    "results_9DoF = get_evaluation_results(configs=configs, algorithm=\"astar\")\n",
    "results_9DoF.extend(get_evaluation_results(configs=configs, algorithm=\"prm\"))\n",
    "\n",
    "plot_results_bar(results=results_9DoF, x_axis_values=algorithms, x_axis_title=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4eb4fa",
   "metadata": {},
   "source": [
    "Lazy PRM is almost always significantly faster than A*. But while A* will always find the optimal solution for the given discretization, Lazy PRM often does not find a solution for high DoF and complex, narrow environments. The length of the paths found by Lazy PRM are almost always significantly longer than the paths found by A*. This is to be expected, because A* finds optimal paths, while Lazy PRM just searches for any path. The few cases where the path found by Lazy PRM is shorter than the path found by A* can be attributed to the fact that A* only changes one DoF per step in this implementation. Lazy PRM can change multiple DoF simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b09d224",
   "metadata": {},
   "source": [
    "#### Example Result A*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5f0c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = results_3DoF[0][1][\"complex\"]\n",
    "solver = result[\"solver\"]\n",
    "environment = solver._collisionChecker\n",
    "solution = result[\"solution\"]\n",
    "animateSolution(planner=solver, environment=environment, solution=solution, dof=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0552f4db",
   "metadata": {},
   "source": [
    "####  Example Result PRM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef44cd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = results_3DoF[1][1][\"complex\"]\n",
    "solver = result[\"solver\"]\n",
    "environment = solver._collisionChecker\n",
    "solution = result[\"solution\"]\n",
    "print(solution)\n",
    "animateSolution(planner=solver, environment=environment, solution=solution, dof=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af24300",
   "metadata": {},
   "source": [
    "Lazy PRM results in smoother motions than A*, because more than one joint can be moved simultaneously. However, the solution path found by A* in most cases is shorter. You can see that the arm in the A* only narrowly avoids the obstacles. This is a symptom of A* only changing the DoF as much as necessary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
