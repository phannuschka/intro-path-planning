{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e88dc04",
   "metadata": {},
   "source": [
    "# Evaluation 2DoF Point Robot\n",
    "\n",
    "In this notebook we evaluate the extended A* using a point robot in a 2 DoF environment. First let us take a look at the environments used for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618f3681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.IPBenchmark import Benchmark \n",
    "import evaluation.two_dof.IPTestSuite_2DoF as ts\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from evaluation.two_dof.PlotEnvironments import visualizeBenchmark\n",
    "\n",
    "#because of time constraints we use a selection of benchmarks instead of the whole list.\n",
    "evaluation_indices = [0, 1, 2, 3, 6, 9]\n",
    "\n",
    "for benchmark in [ts.benchList[i] for i in evaluation_indices]:\n",
    "        benchmark: Benchmark\n",
    "        print(f\"----- benchmark: {benchmark.name} -----\")\n",
    "        fig, ax = visualizeBenchmark(benchmark)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164893e0",
   "metadata": {},
   "source": [
    "## Evaluation Code\n",
    "\n",
    "In this section you can find some code for evaluating A*, loading existing results and plotting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483ffade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import networkx as nx\n",
    "from typing import List, Dict\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), 'core'))\n",
    "sys.path.append(os.path.join(os.getcwd(), 'evaluation/two_dof'))\n",
    "\n",
    "from core.IPAStarExtended import AStar\n",
    "from Benchmarking import get_config_dir_name, evaluate\n",
    "\n",
    "def get_evaluation_results(configs: List[Dict], dump: bool = True):\n",
    "    results = []\n",
    "    for i, config in enumerate(configs):\n",
    "        print(f\"----- config {i + 1} of {len(configs)} -----\")\n",
    "\n",
    "        config_results = {}\n",
    "        benchmarks = [ts.benchList[i] for i in config[\"benchmarks\"]]\n",
    "\n",
    "        for benchmark in benchmarks:\n",
    "            print(f\"----- benchmark: {benchmark.name} -----\")\n",
    "            benchmark_results = {}\n",
    "\n",
    "            dir_name = get_config_dir_name(config=config, benchmark_name=benchmark.name)\n",
    "\n",
    "            if os.path.exists(dir_name):\n",
    "                # load evaluation results if they exist.\n",
    "\n",
    "                astar = AStar(benchmark.collisionChecker)\n",
    "                astar.start = benchmark.startList[0]\n",
    "                astar.goal = benchmark.goalList[0]\n",
    "\n",
    "                with open(f'{dir_name}/stats.json') as f:\n",
    "                    stats = json.load(f)\n",
    "\n",
    "                with open(f'{dir_name}/solution.json') as f:\n",
    "                    solution = json.load(f)\n",
    "\n",
    "                with open(f'{dir_name}/deltas.json') as f:\n",
    "                    deltas = json.load(f)\n",
    "\n",
    "                with open(f'{dir_name}/graph.json') as f:\n",
    "                    graph = nx.node_link_graph(json.load(f))\n",
    "                    astar.graph = graph \n",
    "\n",
    "            else:\n",
    "                # do evaluation\n",
    "\n",
    "                stats, solution, deltas, astar = evaluate(config=config, benchmark=benchmark, dump=dump)\n",
    "\n",
    "            \n",
    "            benchmark_results[\"stats\"] = stats\n",
    "            benchmark_results[\"solution\"] = solution\n",
    "            benchmark_results[\"deltas\"] = deltas\n",
    "            benchmark_results[\"astar\"] = astar\n",
    "\n",
    "            config_results[benchmark.name] = benchmark_results\n",
    "        results.append((config, config_results))\n",
    "    return results    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e648830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_line(results: List, x_axis_values: List, x_axis_title: str):\n",
    "    # plot execution time\n",
    "    plot_lines = {}\n",
    "    for bench_name in results[0][1].keys():\n",
    "        plot_lines[bench_name] = []\n",
    "\n",
    "    for _, config_results in results:\n",
    "        for bench_name, benchmark_results in config_results.items():\n",
    "            plot_lines[bench_name].append(benchmark_results[\"stats\"][\"execution_time\"])\n",
    "\n",
    "    for bench_name in results[0][1].keys():\n",
    "        plt.plot(x_axis_values, plot_lines[bench_name], \"o-\", label = bench_name)\n",
    "    plt.legend()\n",
    "    plt.title(\"Execution Time\")\n",
    "    plt.xlabel(x_axis_title)\n",
    "    plt.ylabel(\"Execution Time [s]\")\n",
    "    plt.show()\n",
    "\n",
    "    # plot roadmap size\n",
    "    plot_lines = {}\n",
    "    for bench_name in results[0][1].keys():\n",
    "        plot_lines[bench_name] = []\n",
    "\n",
    "    for _, config_results in results:\n",
    "        for bench_name, benchmark_results in config_results.items():\n",
    "            plot_lines[bench_name].append(benchmark_results[\"stats\"][\"road_map_size\"])\n",
    "\n",
    "    for bench_name in results[0][1].keys():\n",
    "        plt.plot(x_axis_values, plot_lines[bench_name], \"o-\", label = bench_name)\n",
    "    plt.legend()\n",
    "    plt.title(\"Roadmap Size\")\n",
    "    plt.xlabel(x_axis_title)\n",
    "    plt.ylabel(\"Roadmap Size\")\n",
    "    plt.show()\n",
    "\n",
    "    # plot nume nodes in solution path\n",
    "    plot_lines = {}\n",
    "    for bench_name in results[0][1].keys():\n",
    "        plot_lines[bench_name] = []\n",
    "\n",
    "    for _, config_results in results:\n",
    "        for bench_name, benchmark_results in config_results.items():\n",
    "            if (benchmark_results[\"stats\"][\"num_nodes_solution_path\"] >= 0):\n",
    "                plot_lines[bench_name].append(benchmark_results[\"stats\"][\"num_nodes_solution_path\"])\n",
    "            else:\n",
    "                plot_lines[bench_name].append(np.nan)\n",
    "\n",
    "    for bench_name in results[0][1].keys():\n",
    "        plt.plot(x_axis_values, plot_lines[bench_name], \"o-\", label = bench_name)\n",
    "    plt.legend()\n",
    "    plt.title(\"Number of Nodes in Solution Path\")\n",
    "    plt.xlabel(x_axis_title)\n",
    "    plt.ylabel(\"Number of Nodes in Solution Path\")\n",
    "    plt.show()\n",
    "\n",
    "    # plot solution path length\n",
    "    plot_lines = {}\n",
    "    for bench_name in results[0][1].keys():\n",
    "        plot_lines[bench_name] = []\n",
    "\n",
    "    for _, config_results in results:\n",
    "        for bench_name, benchmark_results in config_results.items():\n",
    "            if (benchmark_results[\"stats\"][\"solution_path_length\"] >= 0):\n",
    "                plot_lines[bench_name].append(benchmark_results[\"stats\"][\"solution_path_length\"])\n",
    "            else:\n",
    "                plot_lines[bench_name].append(np.nan)\n",
    "\n",
    "    for bench_name in results[0][1].keys():\n",
    "        plt.plot(x_axis_values, plot_lines[bench_name], \"o-\", label = bench_name)\n",
    "    plt.legend()\n",
    "    plt.title(\"Solution Path Length\")\n",
    "    plt.xlabel(x_axis_title)\n",
    "    plt.ylabel(\"Solution Path Length\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e2e9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_bar(results: List, x_axis_values: List, x_axis_title: str):\n",
    "    # Get benchmark names\n",
    "    bench_names = list(results[0][1].keys())\n",
    "    n_benchmarks = len(bench_names)\n",
    "    n_configs = len(x_axis_values)\n",
    "    \n",
    "    # Set up bar positioning\n",
    "    bar_width = 0.8 / n_configs  # Total width of 0.8, divided by number of configs\n",
    "    x_positions = np.arange(n_benchmarks)\n",
    "    \n",
    "    # Plot execution time\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, x_val in enumerate(x_axis_values):\n",
    "        execution_times = []\n",
    "        for bench_name in bench_names:\n",
    "            _, config_results = results[i]\n",
    "            execution_times.append(config_results[bench_name][\"stats\"][\"execution_time\"])\n",
    "        \n",
    "        offset = (i - (n_configs - 1) / 2) * bar_width\n",
    "        plt.bar(x_positions + offset, execution_times, bar_width, \n",
    "                label=f'{x_axis_title}: {x_val}', alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Benchmarks')\n",
    "    plt.ylabel('Execution Time [s]')\n",
    "    plt.title('Execution Time by Benchmark')\n",
    "    plt.xticks(x_positions, bench_names, rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot roadmap size\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, x_val in enumerate(x_axis_values):\n",
    "        roadmap_sizes = []\n",
    "        for bench_name in bench_names:\n",
    "            _, config_results = results[i]\n",
    "            roadmap_sizes.append(config_results[bench_name][\"stats\"][\"road_map_size\"])\n",
    "        \n",
    "        offset = (i - (n_configs - 1) / 2) * bar_width\n",
    "        plt.bar(x_positions + offset, roadmap_sizes, bar_width, \n",
    "                label=f'{x_axis_title}: {x_val}', alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Benchmarks')\n",
    "    plt.ylabel('Roadmap Size')\n",
    "    plt.title('Roadmap Size by Benchmark')\n",
    "    plt.xticks(x_positions, bench_names, rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot number of nodes in solution path\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, x_val in enumerate(x_axis_values):\n",
    "        num_nodes = []\n",
    "        for bench_name in bench_names:\n",
    "            _, config_results = results[i]\n",
    "            val = config_results[bench_name][\"stats\"][\"num_nodes_solution_path\"]\n",
    "            num_nodes.append(val if val >= 0 else np.nan)\n",
    "        \n",
    "        offset = (i - (n_configs - 1) / 2) * bar_width\n",
    "        plt.bar(x_positions + offset, num_nodes, bar_width, \n",
    "                label=f'{x_axis_title}: {x_val}', alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Benchmarks')\n",
    "    plt.ylabel('Number of Nodes in Solution Path')\n",
    "    plt.title('Number of Nodes in Solution Path by Benchmark')\n",
    "    plt.xticks(x_positions, bench_names, rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot solution path length\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i, x_val in enumerate(x_axis_values):\n",
    "        path_lengths = []\n",
    "        for bench_name in bench_names:\n",
    "            _, config_results = results[i]\n",
    "            val = config_results[bench_name][\"stats\"][\"solution_path_length\"]\n",
    "            path_lengths.append(val if val >= 0 else np.nan)\n",
    "        \n",
    "        offset = (i - (n_configs - 1) / 2) * bar_width\n",
    "        plt.bar(x_positions + offset, path_lengths, bar_width, \n",
    "                label=f'{x_axis_title}: {x_val}', alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Benchmarks')\n",
    "    plt.ylabel('Solution Path Length')\n",
    "    plt.title('Solution Path Length by Benchmark')\n",
    "    plt.xticks(x_positions, bench_names, rotation=45, ha='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00761fe",
   "metadata": {},
   "source": [
    "## Evaluation: Discretization\n",
    "\n",
    "In this section we will evaluate the influence of the discretization on A*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe8456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "disc_values = [10, 20, 50, 100, 200]\n",
    "\n",
    "configs = []\n",
    "\n",
    "discretization_config = dict()\n",
    "discretization_config[\"w\"] = 0.5\n",
    "discretization_config[\"heuristic\"]  = \"euclidean\"\n",
    "discretization_config[\"reopen\"] = True\n",
    "discretization_config[\"dof\"] = 2\n",
    "discretization_config[\"check_connection\"] = True\n",
    "discretization_config[\"lazy_check_connection\"] = True\n",
    "discretization_config[\"benchmarks\"] = [0, 1, 2, 3, 6, 9]\n",
    "\n",
    "for discretization_value in disc_values:\n",
    "    config = copy.deepcopy(discretization_config)\n",
    "    config[\"discretization\"] = [discretization_value for _ in range(config[\"dof\"])]\n",
    "    configs.append(config)\n",
    "\n",
    "results = get_evaluation_results(configs=configs)\n",
    "\n",
    "plot_results_line(results=results, x_axis_values=disc_values, x_axis_title=\"Discretization Steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97168e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show results\n",
    "\n",
    "from IPVISAStar import aStarVisualize\n",
    "\n",
    "for i, (_, config_results) in enumerate(results):\n",
    "    fig = plt.figure(figsize=(35,10))\n",
    "    for j, (bench_name, benchmark_results) in enumerate(config_results.items()):\n",
    "        astar = benchmark_results[\"astar\"]\n",
    "        solution = benchmark_results[\"solution\"]\n",
    "        ax = fig.add_subplot(1,len(config_results.items()),j + 1)\n",
    "        aStarVisualize(planner=astar, solution=solution, ax=ax, nodeSize=(1000 / disc_values[i]), viz_solution=(len(solution) > 0))\n",
    "    fig.suptitle(f\"Discretization: {disc_values[i]}\", fontsize=50)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d295192",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Increasing the number of dicretization steps signinficantly increases the execution time. Interestingly the path length only shows significant improvements between low numbers of discretization steps. For larger numbers of discretization steps the relative improvements decrease. This is because with larger numbers of discretization steps only smaller and smaller improvemnts can be gained by staying closer to obstacles. Large improvements only happen, when completely new paths can be found due to the finer discretization. A finer discretization does however enable A* to find solutions for benchmarks that are impossible to solve with a curser dicretization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c96bcf",
   "metadata": {},
   "source": [
    "## Evaluation: Weight w\n",
    "\n",
    "In this section we will evaluate the influence of the weight w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce73d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "w_values = [0.5, 0.75, 1.0]\n",
    "\n",
    "configs = []\n",
    "\n",
    "weight_config = dict()\n",
    "weight_config[\"heuristic\"] = \"euclidean\"\n",
    "weight_config[\"reopen\"] = True\n",
    "weight_config[\"dof\"] = 2\n",
    "weight_config[\"discretization\"] = [50 for _ in range(weight_config[\"dof\"])]\n",
    "weight_config[\"check_connection\"] = True\n",
    "weight_config[\"lazy_check_connection\"] = True\n",
    "weight_config[\"benchmarks\"] = [0, 1, 2, 3, 6, 9]\n",
    "\n",
    "for w_value in w_values:\n",
    "    config = copy.deepcopy(weight_config)\n",
    "    config[\"w\"] = w_value\n",
    "    configs.append(config)\n",
    "\n",
    "results = get_evaluation_results(configs=configs)\n",
    "\n",
    "plot_results_line(results=results, x_axis_values=w_values, x_axis_title=\"Weight w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f2e239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show results\n",
    "\n",
    "from IPVISAStar import aStarVisualize\n",
    "\n",
    "for i, (_, config_results) in enumerate(results):\n",
    "    fig = plt.figure(figsize=(35,10))\n",
    "    for j, (bench_name, benchmark_results) in enumerate(config_results.items()):\n",
    "        astar = benchmark_results[\"astar\"]\n",
    "        solution = benchmark_results[\"solution\"]\n",
    "        ax = fig.add_subplot(1,len(config_results.items()),j + 1)\n",
    "        aStarVisualize(planner=astar, solution=solution, ax=ax, nodeSize=(1000 / disc_values[i]), viz_solution=(len(solution) > 0))\n",
    "    fig.suptitle(f\"Weight: {w_values[i]}\", fontsize=50)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b846a4",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "While a higher weight factor w causes some increases in the number of nodes in the solution path the decrease in roadmap size is much more significant. This means that increasing the weight factor w is an effective tool to increase the search efficiency with only small drawbacks in some cases (e.g. trap near the start of the mikado benchmark)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15b7a8a",
   "metadata": {},
   "source": [
    "## Evaluation: Line Collision\n",
    "\n",
    "In this section we will evaluate the influence of line collision detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deebc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "w_values = [0.5, 0.75, 1.0]\n",
    "\n",
    "configs = []\n",
    "\n",
    "line_coll_config = dict()\n",
    "line_coll_config[\"heuristic\"] = \"euclidean\"\n",
    "line_coll_config[\"reopen\"] = True\n",
    "line_coll_config[\"dof\"] = 2\n",
    "line_coll_config[\"discretization\"] = [50 for _ in range(line_coll_config[\"dof\"])]\n",
    "line_coll_config[\"w\"] = .5\n",
    "line_coll_config[\"benchmarks\"] = [0, 1, 2, 3, 6, 9]\n",
    "\n",
    "config = copy.deepcopy(line_coll_config)\n",
    "config[\"check_connection\"] = True\n",
    "config[\"lazy_check_connection\"] = True\n",
    "configs.append(config)\n",
    "\n",
    "config = copy.deepcopy(line_coll_config)\n",
    "config[\"check_connection\"] = False\n",
    "config[\"lazy_check_connection\"] = False\n",
    "configs.append(config)\n",
    "\n",
    "results = get_evaluation_results(configs=configs)\n",
    "\n",
    "plot_results_bar(results=results, x_axis_values=[True, False], x_axis_title=\"Line Collision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a4d9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show results\n",
    "\n",
    "from IPVISAStar import aStarVisualize\n",
    "\n",
    "activation_values = [True, False]\n",
    "\n",
    "for i, (_, config_results) in enumerate(results):\n",
    "    fig = plt.figure(figsize=(35,10))\n",
    "    for j, (bench_name, benchmark_results) in enumerate(config_results.items()):\n",
    "        astar = benchmark_results[\"astar\"]\n",
    "        solution = benchmark_results[\"solution\"]\n",
    "        ax = fig.add_subplot(1,len(config_results.items()),j + 1)\n",
    "        aStarVisualize(planner=astar, solution=solution, ax=ax, nodeSize=(1000 / disc_values[i]), viz_solution=(len(solution) > 0))\n",
    "    fig.suptitle(f\"Line Collision Detection: {activation_values[i]}\", fontsize=50)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b165bd",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Enabling line collision detection leads to a significant increase in execution time. However, line collision can prevent incorrect solutions that lead to collisions with obstacles. When comparing the search time with line collision detection enabled versus to the search time with an increased number of discretization steps that prevents obstacle collisions in our benchmarks, we conclude that increasing the number of discretization steps is more computationally efficient. But if the size of obstacles in your configuration space is unknown, enabling line collision detection can still be worthwile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b417d86",
   "metadata": {},
   "source": [
    "## Evaluation: Reopening\n",
    "\n",
    "In this section we will evaluate the influence of reopening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1513b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "w_values = [0.5, 0.75, 1.0]\n",
    "\n",
    "configs = []\n",
    "\n",
    "reopening_config = dict()\n",
    "reopening_config[\"heuristic\"] = \"euclidean\"\n",
    "reopening_config[\"reopen\"] = True\n",
    "reopening_config[\"dof\"] = 2\n",
    "reopening_config[\"discretization\"] = [50 for _ in range(reopening_config[\"dof\"])]\n",
    "reopening_config[\"w\"] = .75\n",
    "reopening_config[\"benchmarks\"] = [0, 1, 2, 3, 6, 9]\n",
    "reopening_config[\"check_connection\"] = True\n",
    "reopening_config[\"lazy_check_connection\"] = True\n",
    "\n",
    "config = copy.deepcopy(reopening_config)\n",
    "config[\"reopen\"] = True\n",
    "configs.append(config)\n",
    "\n",
    "config = copy.deepcopy(reopening_config)\n",
    "config[\"reopen\"] = False\n",
    "configs.append(config)\n",
    "\n",
    "results = get_evaluation_results(configs=configs)\n",
    "\n",
    "plot_results_bar(results=results, x_axis_values=[True, False], x_axis_title=\"Reopening\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3255d072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show results\n",
    "\n",
    "from IPVISAStar import aStarVisualize\n",
    "\n",
    "activation_values = [True, False]\n",
    "\n",
    "for i, (_, config_results) in enumerate(results):\n",
    "    fig = plt.figure(figsize=(35,10))\n",
    "    for j, (bench_name, benchmark_results) in enumerate(config_results.items()):\n",
    "        astar = benchmark_results[\"astar\"]\n",
    "        solution = benchmark_results[\"solution\"]\n",
    "        ax = fig.add_subplot(1,len(config_results.items()),j + 1)\n",
    "        aStarVisualize(planner=astar, solution=solution, ax=ax, nodeSize=(1000 / disc_values[i]), viz_solution=(len(solution) > 0))\n",
    "    fig.suptitle(f\"Reopening: {activation_values[i]}\", fontsize=50)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7d4c57",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Enabling reopening does not significantly improve the path length in any of our benchmarks (although some changes can be seen when we look at the solutions of the mikado benchmark). However, it does lead to a significant deterioration of the execution time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
